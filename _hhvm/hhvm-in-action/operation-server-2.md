---
layout: book_page
title:  "运维篇-上线"
date:   2015-1-7 08:34:04
---

## 2.	服务安全

HHVM的web运行方式和PHP不同，是采用单进程多线程的模式，多线程模型有他的好处就是消耗资源少、节约内存、并发性高，但是也有他的劣势，那么就是线程方式容易出现线程安全问题、内存泄露、crash后整个进程退出；
虽然有如上弊端，但是我们可以通过一些方式进行规避，至少目前在百度的运维过程中，这些方案还是比较稳定的，如下我们通过几种方式进行避免和定位一些线上异常现象：

### 1. crash问题
hhvm 如果出现crash后，其实对于服务来说是很危险的，相对PHP的多进程模式crash一个slave进程无所谓，但是如果hhvm crash了，那么整个进程就退出了，那么我们如何避免这种问题呢？
我们用一些工具，或者自己写一些监控工具，监测HHVM的进程存活状态（如surpervise），当进程crash后即时拉起避免期间所带来的流量损失，但是crash必定会造成一定的流量丢失，而且HHVM还会有预热过程（JIT翻译时一般比直接解释执行还要慢不少），那么我们如何针对这些问题进行避免呢？

  1.	web server（如nginx、lighttpd等）代理hhvm，通过策略调度到备机
    
  通过upstream或者error_page等方式切换到备机上,初期的话备机是ZEND虚拟机，但是后期也打算备机也换成HHVM，由于当HHVM crash后，切换到zend后会出现cpu瞬间过高或者打满cpu的现象，所以为了避免这种情况，可以预先预热好一个HHVM作为备机，然后采用这种双buffer的机制进行运行，这种情况下就可以在HHVM crash的情况下也可以保证流量不丢失。
    
  1. 内存泄露问题和线程安全问题将在如下2篇详解

    
### 2. 内存泄露问题


  内存泄露对于多线程模式还是比较常见的，一般在上线前我们会针对内存泄露进行一系列的测试，但是如果线上出现了内存泄露我们如何处理呢？
  具体分析过程我们已经在内存泄露分析篇中进行介绍这里不进行详细阐述，我们主要针对内存泄露在线上出现时一些处理方式：
  
  1.	内存监控报警
  1.	设置MaxRss进行内存约束，超过内存后HHVM退出，通过supervise拉起
  1.	线下复现和分析内存泄露
  
### 3.线程安全问题
  
  线程安全问题在多线程模式下也是一种比较常见的现象，而且此种问题比较危险，线程安全问题一般会出现如下现象：
  
  1.	crash
  1.	死锁
  1.	信息不一致
  
  那么我们将介绍下如何针对上面3种情况在线上的解决方式：
  
####	3.1. Crash
  
  线程安全问题中，crash应该是最好解决的一种，因为crash后，我们可以直接看到栈，然后分析究竟是哪里出现了问题，一般线程安全crash都是针对全局的静态变量操作并且销毁时会遇到此种问题，如onig库默认是单进程模式，如果此类问题，我们只需要查看crash 栈即可分析。
  
#### 3.2.	死锁
  
  死锁问题是线上遇到问题的一种棘手的问题，但是也还好分析，为何说棘手呢？因为出现死锁时，会造成大范围请求的延迟，无法接收新的请求，那么我们如何分析线上是否出现了死锁呢？
  
##### 3.2.1日志监控

Nginx 的请求源源不断，但是hhvm的acess日志却不进行接受任何请求（日志已经不输出），并且HHVM的超时机制已经失效，不会再放开worker线程

##### 3.2.2监控探活

主动定期对url进行探活,在一定失败次数后进行报警，查看是否死锁。

或者观察admin server是否请求失败，如果admin server请求失败（一般adminserver无压力），这样的情况下一般都是hhvm死锁了

那么针对死锁的情况我们可以有几种定位方式呢？

我们可以通过gdb attach，strace，和系统abort查看core栈，由于hhvm是多线程模式，所以我们需要找到死锁的具体线程，抓主进程是没有用的，下面用一种方式很实用，就是查看线程的cpu使用率来进行判断：

    ps -mp <PID> -o THREAD,tid,time|sort

通过排序我们可以查看到cpu使用率最高的线程，然后我们查看到tid后，我们通过如下3种方式进行跟踪

  1. attach
  
        gdb attach tid
  
  通过gdb命令直接attach到怀疑死锁的线程让，然后通过bt命令查看具体在哪个位置死锁了；
  
  **注：**
  
  使用的gdb需要和编译hhvm的版本一致，一般系统默认版本的都是较低的版本

  1.	strace
  
        strace –p tid –o trace.log
  
  通过strace命令可以查看到系统的调用，这种方式的好处就是strace中，程序还是可以继续运行的，而通过gdb则会暂时中断该线程，但是strace查看的都是系统调用而非程序栈，一般来说可能定位上会比较困难。

  1.	abort
  
        kill -SIGABRT <PID>或<TID>
  
  这种方式一般用在线上不方便gdb，而且对于时效较高的系统，这样将程序crash后，然后去查看coredump的栈也可以进行分析；
  
  **优点：**crash后重新拉起，不会造成程序中断，而且可以看到详细栈
  
  **缺点：**如果分析错误了线程，现场将会失去，如果不好复现问题则不容易定位

####	3.3 cpu异常过高

cpu过高现象有时候可能是使用了动态语法，也有可能是线程内部出现了死循环，其实这种现象的跟踪一般可以参考死锁问题的方式，但是唯一不同的是，死锁时请求无法访问，cpu高时请求可以访问，如果是eval语法可以参见动态语法分析，如果是死循环可以查看在无访问时监控接口是否有load，如果load持续保持一般是死循环问题（如preg出现过）。

#### 3.4 信息不一致
线程安全也会造成一些信息不一致的问题，比如一些全局变量非原子操作，这种问题相对上面的问题就很难发现了，其实一般这种问题在销毁信息时很可能出现crash，但是如果一直是插入不销毁，那么就很难发现了，这种问题一定要在测试回归时注意，但是目前来讲，hhvm还未发现此种问题，这种问题一般是多线程操作新手容易出现的问题。
